{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMShTX80Kz/LZIcUMe9U8y/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manglesh001/DL-assigment1/blob/main/DL_Ass1_Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rLRUvo50IogF"
      },
      "outputs": [],
      "source": [
        "#import all necessary files\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion-MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "gUXKOGA-JKbM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "Lb7q3rnTJLEW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the 28x28 images into 784-dimensional vectors\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)"
      ],
      "metadata": {
        "id": "m4yGKEQBJLGy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test = encoder.transform(y_test.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "W4hdatkuJLJC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ROlxIgGULFJT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  #softmax  activation function ouput layer\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "  #Sigmoid activation function hidden layer\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "t6veP94YLIN6",
        "outputId": "5802c714-709e-4875-eadb-a0c202ef5cb6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-21-71325d94b14f>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-71325d94b14f>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def softmax(self, x):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "HP3YoA05LK9d",
        "outputId": "053b477f-948c-49a7-9038-04b08f55e834"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-22-c598042dfbce>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-c598042dfbce>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def forward(self, x):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8A_n86ybLOh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MQ0GSdftLOkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AwoHRrxlLOnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class FFNN\n",
        "class FFNN:\n",
        "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "    # Initialize weights and biases\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01)\n",
        "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
        "\n",
        "  #softmax  activation function ouput layer\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "  #Sigmoid activation function hidden layer\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "\n",
        "  #forward Pass FFNN\n",
        "    def forward(self, x):\n",
        "        self.activations = [x]\n",
        "        self.z_values = []\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            if i == len(self.weights) - 1:\n",
        "                # Output layer uses softmax\n",
        "                activation = self.softmax(z)\n",
        "            else:\n",
        "                # Hidden layers use sigmoid\n",
        "                activation = self.sigmoid(z)\n",
        "            self.activations.append(activation)\n",
        "      # Return the output probabilities\n",
        "        return self.activations[-1]\n",
        "\n",
        "    #Backward Pass FFNN\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        m = x.shape[0]\n",
        "        self.deltas = [None] * len(self.weights)\n",
        "\n",
        "        # Output layer error\n",
        "        output_error = self.activations[-1] - y\n",
        "        self.deltas[-1] = output_error\n",
        "\n",
        "        # Backpropagate errors\n",
        "        for i in range(len(self.weights) - 2, -1, -1):\n",
        "            error = np.dot(self.deltas[i + 1], self.weights[i + 1].T)\n",
        "            self.deltas[i] = error * self.sigmoid_derivative(self.activations[i + 1])\n",
        "\n",
        "        # Update weights and biases\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= self.learning_rate * np.dot(self.activations[i].T, self.deltas[i]) / m\n",
        "            self.biases[i] -= self.learning_rate * np.sum(self.deltas[i], axis=0, keepdims=True) / m\n",
        "\n",
        "    def train(self, x, y, epochs=10, batch_size=32):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, x.shape[0], batch_size):\n",
        "                x_batch = x[i:i + batch_size]\n",
        "                y_batch = y[i:i + batch_size]\n",
        "\n",
        "                # Forward pass\n",
        "                self.forward(x_batch)\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(x_batch, y_batch)\n",
        "\n",
        "            # Print loss at  every epoch\n",
        "            predictions = self.forward(x)\n",
        "            loss = -np.mean(y * np.log(predictions + 1e-10))\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Return the softmax probabilities for the input\n",
        "\n",
        "    def predict_probabilities(self, x):\n",
        "        return self.forward(x)"
      ],
      "metadata": {
        "id": "lat0hngQJLLL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the FFNN architecture\n",
        "layer_sizes = [784, 128, 64, 10]\n",
        "model = FFNN(layer_sizes, learning_rate=0.1)"
      ],
      "metadata": {
        "id": "UjHU4_2qJLNo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.train(x_train, y_train, epochs=10, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvjJl-VCJLQF",
        "outputId": "39cc23c9-774b-4044-857c-2f887af87b34"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.1898\n",
            "Epoch 2/10, Loss: 0.0933\n",
            "Epoch 3/10, Loss: 0.0684\n",
            "Epoch 4/10, Loss: 0.0581\n",
            "Epoch 5/10, Loss: 0.0511\n",
            "Epoch 6/10, Loss: 0.0470\n",
            "Epoch 7/10, Loss: 0.0440\n",
            "Epoch 8/10, Loss: 0.0418\n",
            "Epoch 9/10, Loss: 0.0401\n",
            "Epoch 10/10, Loss: 0.0387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the probability distribution for a sample from the test set to 10th index\n",
        "sample_index = 10\n",
        "sample = x_test[sample_index].reshape(1, -1)\n",
        "probabilities = model.predict_probabilities(sample)"
      ],
      "metadata": {
        "id": "ZZSWvYajMQPd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the probability distribution of each class\n",
        "print(\"Probability distribution for the sample at 10 Index:\")\n",
        "for i, prob in enumerate(probabilities[0]):\n",
        "    print(f\"Class {i}: {prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KNNyhbrJLS5",
        "outputId": "92940e29-e314-429d-d283-0a93eb06a909"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability distribution for the sample at 10 Index:\n",
            "Class 0: 0.0003\n",
            "Class 1: 0.0001\n",
            "Class 2: 0.1257\n",
            "Class 3: 0.0009\n",
            "Class 4: 0.8333\n",
            "Class 5: 0.0000\n",
            "Class 6: 0.0392\n",
            "Class 7: 0.0000\n",
            "Class 8: 0.0004\n",
            "Class 9: 0.0000\n"
          ]
        }
      ]
    }
  ]
}