# -*- coding: utf-8 -*-
"""DL1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MP026MPpW1cpy9thrbA6wY7f0ji8YDEz
"""

import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import fashion_mnist

# Load the Fashion-MNIST dataset
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Class names for Fashion-MNIST
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Create a figure to plot the images
plt.figure(figsize=(10, 15))
plt.suptitle("Fashion-MNIST DataSet", fontsize=16)

# Plot one sample image for each class
for i in range(len(class_names)):
    # Find the first occurrence of each class in the training set
    idx = np.where(y_train == i)[0][1]

    # Plot the image
    plt.subplot(5, 5, i + 1)
    plt.imshow(x_train[idx], cmap='gray')
    plt.title(class_names[i])
    plt.axis('on')

# Adjust layout and display the plot
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import fashion_mnist

import numpy as np
from keras.datasets import fashion_mnist
from sklearn.preprocessing import OneHotEncoder

# Load the Fashion-MNIST dataset
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Normalize pixel values to [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Flatten the 28x28 images into 784-dimensional vectors
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# One-hot encode the labels
encoder = OneHotEncoder(sparse_output=False)
y_train = encoder.fit_transform(y_train.reshape(-1, 1))
y_test = encoder.transform(y_test.reshape(-1, 1))

# Define the neural network
class FeedforwardNeuralNetwork:
    def __init__(self, layer_sizes, learning_rate=0.01):
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.weights = []
        self.biases = []

        # Initialize weights and biases
        for i in range(len(layer_sizes) - 1):
            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01)
            self.biases.append(np.zeros((1, layer_sizes[i + 1])))

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def forward(self, x):
        self.activations = [x]
        self.z_values = []

        for i in range(len(self.weights)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)
            if i == len(self.weights) - 1:
                # Output layer uses softmax
                activation = self.softmax(z)
            else:
                # Hidden layers use sigmoid
                activation = self.sigmoid(z)
            self.activations.append(activation)

        return self.activations[-1]

    def backward(self, x, y):
        m = x.shape[0]
        self.deltas = [None] * len(self.weights)

        # Output layer error
        output_error = self.activations[-1] - y
        self.deltas[-1] = output_error

        # Backpropagate errors
        for i in range(len(self.weights) - 2, -1, -1):
            error = np.dot(self.deltas[i + 1], self.weights[i + 1].T)
            self.deltas[i] = error * self.sigmoid_derivative(self.activations[i + 1])

        # Update weights and biases
        for i in range(len(self.weights)):
            self.weights[i] -= self.learning_rate * np.dot(self.activations[i].T, self.deltas[i]) / m
            self.biases[i] -= self.learning_rate * np.sum(self.deltas[i], axis=0, keepdims=True) / m

    def train(self, x, y, epochs=10, batch_size=32):
        for epoch in range(epochs):
            for i in range(0, x.shape[0], batch_size):
                x_batch = x[i:i + batch_size]
                y_batch = y[i:i + batch_size]

                # Forward pass
                self.forward(x_batch)

                # Backward pass
                self.backward(x_batch, y_batch)

            # Print loss every epoch
            predictions = self.forward(x)
            loss = -np.mean(y * np.log(predictions + 1e-10))
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}")

    def predict(self, x):
        return np.argmax(self.forward(x), axis=1)

# Define the network architecture
layer_sizes = [784, 128, 64, 10]  # Input layer: 784, Hidden layers: 128 and 64, Output layer: 10
model = FeedforwardNeuralNetwork(layer_sizes, learning_rate=0.1)

# Train the model
model.train(x_train, y_train, epochs=10, batch_size=32)

# Evaluate the model
predictions = model.predict(x_test)
accuracy = np.mean(predictions == np.argmax(y_test, axis=1))
print(f"Test Accuracy: {accuracy * 100:.2f}%")

import numpy as np
from keras.datasets import fashion_mnist
from sklearn.preprocessing import OneHotEncoder

# Load the Fashion-MNIST dataset
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Normalize pixel values to [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Flatten the 28x28 images into 784-dimensional vectors
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# One-hot encode the labels
encoder = OneHotEncoder(sparse_output=False)
y_train = encoder.fit_transform(y_train.reshape(-1, 1))
y_test = encoder.transform(y_test.reshape(-1, 1))

# Define the neural network
class FeedforwardNeuralNetwork:
    def __init__(self, layer_sizes, learning_rate=0.01):
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.weights = []
        self.biases = []

        # Initialize weights and biases
        for i in range(len(layer_sizes) - 1):
            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01)
            self.biases.append(np.zeros((1, layer_sizes[i + 1])))

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def forward(self, x):
        self.activations = [x]
        self.z_values = []

        for i in range(len(self.weights)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)
            if i == len(self.weights) - 1:
                # Output layer uses softmax
                activation = self.softmax(z)
            else:
                # Hidden layers use sigmoid
                activation = self.sigmoid(z)
            self.activations.append(activation)

        return self.activations[-1]  # Return the output probabilities

    def backward(self, x, y):
        m = x.shape[0]
        self.deltas = [None] * len(self.weights)

        # Output layer error
        output_error = self.activations[-1] - y
        self.deltas[-1] = output_error

        # Backpropagate errors
        for i in range(len(self.weights) - 2, -1, -1):
            error = np.dot(self.deltas[i + 1], self.weights[i + 1].T)
            self.deltas[i] = error * self.sigmoid_derivative(self.activations[i + 1])

        # Update weights and biases
        for i in range(len(self.weights)):
            self.weights[i] -= self.learning_rate * np.dot(self.activations[i].T, self.deltas[i]) / m
            self.biases[i] -= self.learning_rate * np.sum(self.deltas[i], axis=0, keepdims=True) / m

    def train(self, x, y, epochs=10, batch_size=32):
        for epoch in range(epochs):
            for i in range(0, x.shape[0], batch_size):
                x_batch = x[i:i + batch_size]
                y_batch = y[i:i + batch_size]

                # Forward pass
                self.forward(x_batch)

                # Backward pass
                self.backward(x_batch, y_batch)

            # Print loss every epoch
            predictions = self.forward(x)
            loss = -np.mean(y * np.log(predictions + 1e-10))
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}")

    def predict_probabilities(self, x):
        # Return the softmax probabilities for the input
        return self.forward(x)

# Define the network architecture
layer_sizes = [784, 128, 64, 10]  # Input layer: 784, Hidden layers: 128 and 64, Output layer: 10
model = FeedforwardNeuralNetwork(layer_sizes, learning_rate=0.1)

# Train the model
model.train(x_train, y_train, epochs=10, batch_size=32)

# Get the probability distribution for a sample from the test set
sample_index = 0  # Change this to any index in the test set
sample = x_test[sample_index].reshape(1, -1)  # Reshape to (1, 784)
probabilities = model.predict_probabilities(sample)

# Print the probability distribution
print("Probability distribution for the sample:")
for i, prob in enumerate(probabilities[0]):
    print(f"Class {i}: {prob:.4f}")

import numpy as np
from sklearn.preprocessing import OneHotEncoder
from keras.datasets import fashion_mnist

# Load the Fashion-MNIST dataset
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Normalize pixel values to [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Flatten the 28x28 images into 784-dimensional vectors
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# One-hot encode the labels
encoder = OneHotEncoder(sparse_output=False)
y_train = encoder.fit_transform(y_train.reshape(-1, 1))
y_test = encoder.transform(y_test.reshape(-1, 1))

# Define the neural network
class FeedforwardNeuralNetwork:
    def __init__(self, layer_sizes, learning_rate=0.01, optimizer='sgd', **optimizer_params):
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.optimizer = optimizer
        self.optimizer_params = optimizer_params

        # Initialize weights and biases
        self.weights = []
        self.biases = []
        for i in range(len(layer_sizes) - 1):
            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01)
            self.biases.append(np.zeros((1, layer_sizes[i + 1])))

        # Initialize optimizer-specific parameters
        self.initialize_optimizer()

    def initialize_optimizer(self):
        if self.optimizer == 'momentum':
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
        elif self.optimizer == 'nesterov':
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
        elif self.optimizer == 'rmsprop':
            self.s_weights = [np.zeros_like(w) for w in self.weights]
            self.s_biases = [np.zeros_like(b) for b in self.biases]
        elif self.optimizer == 'adam':
            self.m_weights = [np.zeros_like(w) for w in self.weights]
            self.m_biases = [np.zeros_like(b) for b in self.biases]
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
            self.t = 0
        elif self.optimizer == 'nadam':
            self.m_weights = [np.zeros_like(w) for w in self.weights]
            self.m_biases = [np.zeros_like(b) for b in self.biases]
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
            self.t = 0

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def forward(self, x):
        self.activations = [x]
        self.z_values = []

        for i in range(len(self.weights)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)
            if i == len(self.weights) - 1:
                # Output layer uses softmax
                activation = self.softmax(z)
            else:
                # Hidden layers use sigmoid
                activation = self.sigmoid(z)
            self.activations.append(activation)

        return self.activations[-1]

    def backward(self, x, y):
        m = x.shape[0]
        self.deltas = [None] * len(self.weights)

        # Output layer error
        output_error = self.activations[-1] - y
        self.deltas[-1] = output_error

        # Backpropagate errors
        for i in range(len(self.weights) - 2, -1, -1):
            error = np.dot(self.deltas[i + 1], self.weights[i + 1].T)
            self.deltas[i] = error * self.sigmoid_derivative(self.activations[i + 1])

        # Update weights and biases using the chosen optimizer
        self.update_parameters(m)

    def update_parameters(self, m):
        if self.optimizer == 'sgd':
            for i in range(len(self.weights)):
                self.weights[i] -= self.learning_rate * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.biases[i] -= self.learning_rate * np.sum(self.deltas[i], axis=0, keepdims=True) / m
        elif self.optimizer == 'momentum':
            beta = self.optimizer_params.get('beta', 0.9)
            for i in range(len(self.weights)):
                self.v_weights[i] = beta * self.v_weights[i] + (1 - beta) * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.v_biases[i] = beta * self.v_biases[i] + (1 - beta) * np.sum(self.deltas[i], axis=0, keepdims=True) / m
                self.weights[i] -= self.learning_rate * self.v_weights[i]
                self.biases[i] -= self.learning_rate * self.v_biases[i]
        elif self.optimizer == 'nesterov':
            beta = self.optimizer_params.get('beta', 0.9)
            for i in range(len(self.weights)):
                v_weights_prev = self.v_weights[i]
                v_biases_prev = self.v_biases[i]
                self.v_weights[i] = beta * self.v_weights[i] + (1 - beta) * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.v_biases[i] = beta * self.v_biases[i] + (1 - beta) * np.sum(self.deltas[i], axis=0, keepdims=True) / m
                self.weights[i] -= self.learning_rate * (beta * v_weights_prev + (1 - beta) * self.v_weights[i])
                self.biases[i] -= self.learning_rate * (beta * v_biases_prev + (1 - beta) * self.v_biases[i])
        elif self.optimizer == 'rmsprop':
            beta = self.optimizer_params.get('beta', 0.9)
            epsilon = self.optimizer_params.get('epsilon', 1e-8)
            for i in range(len(self.weights)):
                self.s_weights[i] = beta * self.s_weights[i] + (1 - beta) * np.square(np.dot(self.activations[i].T, self.deltas[i]) / m)
                self.s_biases[i] = beta * self.s_biases[i] + (1 - beta) * np.square(np.sum(self.deltas[i], axis=0, keepdims=True) / m)
                self.weights[i] -= self.learning_rate * (np.dot(self.activations[i].T, self.deltas[i]) / m) / (np.sqrt(self.s_weights[i]) + epsilon)
                self.biases[i] -= self.learning_rate * (np.sum(self.deltas[i], axis=0, keepdims=True) / m) / (np.sqrt(self.s_biases[i]) + epsilon)
        elif self.optimizer == 'adam':
            beta1 = self.optimizer_params.get('beta1', 0.9)
            beta2 = self.optimizer_params.get('beta2', 0.999)
            epsilon = self.optimizer_params.get('epsilon', 1e-8)
            self.t += 1
            for i in range(len(self.weights)):
                self.m_weights[i] = beta1 * self.m_weights[i] + (1 - beta1) * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.m_biases[i] = beta1 * self.m_biases[i] + (1 - beta1) * np.sum(self.deltas[i], axis=0, keepdims=True) / m
                self.v_weights[i] = beta2 * self.v_weights[i] + (1 - beta2) * np.square(np.dot(self.activations[i].T, self.deltas[i]) / m)
                self.v_biases[i] = beta2 * self.v_biases[i] + (1 - beta2) * np.square(np.sum(self.deltas[i], axis=0, keepdims=True) / m)
                m_weights_hat = self.m_weights[i] / (1 - beta1 ** self.t)
                m_biases_hat = self.m_biases[i] / (1 - beta1 ** self.t)
                v_weights_hat = self.v_weights[i] / (1 - beta2 ** self.t)
                v_biases_hat = self.v_biases[i] / (1 - beta2 ** self.t)
                self.weights[i] -= self.learning_rate * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)
                self.biases[i] -= self.learning_rate * m_biases_hat / (np.sqrt(v_biases_hat) + epsilon)
        elif self.optimizer == 'nadam':
            beta1 = self.optimizer_params.get('beta1', 0.9)
            beta2 = self.optimizer_params.get('beta2', 0.999)
            epsilon = self.optimizer_params.get('epsilon', 1e-8)
            self.t += 1
            for i in range(len(self.weights)):
                self.m_weights[i] = beta1 * self.m_weights[i] + (1 - beta1) * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.m_biases[i] = beta1 * self.m_biases[i] + (1 - beta1) * np.sum(self.deltas[i], axis=0, keepdims=True) / m
                self.v_weights[i] = beta2 * self.v_weights[i] + (1 - beta2) * np.square(np.dot(self.activations[i].T, self.deltas[i]) / m)
                self.v_biases[i] = beta2 * self.v_biases[i] + (1 - beta2) * np.square(np.sum(self.deltas[i], axis=0, keepdims=True) / m)
                m_weights_hat = (beta1 * self.m_weights[i] / (1 - beta1 ** self.t)) + ((1 - beta1) * np.dot(self.activations[i].T, self.deltas[i]) / m) / (1 - beta1 ** self.t)
                m_biases_hat = (beta1 * self.m_biases[i] / (1 - beta1 ** self.t)) + ((1 - beta1) * np.sum(self.deltas[i], axis=0, keepdims=True) / m) / (1 - beta1 ** self.t)
                v_weights_hat = self.v_weights[i] / (1 - beta2 ** self.t)
                v_biases_hat = self.v_biases[i] / (1 - beta2 ** self.t)
                self.weights[i] -= self.learning_rate * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)
                self.biases[i] -= self.learning_rate * m_biases_hat / (np.sqrt(v_biases_hat) + epsilon)

    def train(self, x, y, epochs=10, batch_size=32):
        for epoch in range(epochs):
            for i in range(0, x.shape[0], batch_size):
                x_batch = x[i:i + batch_size]
                y_batch = y[i:i + batch_size]

                # Forward pass
                self.forward(x_batch)

                # Backward pass
                self.backward(x_batch, y_batch)

            # Print loss every epoch
            predictions = self.forward(x)
            loss = -np.mean(y * np.log(predictions + 1e-10))
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}")

    def predict(self, x):
        return np.argmax(self.forward(x), axis=1)

# Example usage
layer_sizes = [784, 128, 64, 10]  # Input layer: 784, Hidden layers: 128 and 64, Output layer: 10
optimizer = 'adam'  # Choose from 'sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam'
model = FeedforwardNeuralNetwork(layer_sizes, learning_rate=0.001, optimizer=optimizer, beta1=0.9, beta2=0.999, epsilon=1e-8)

# Train the model
model.train(x_train, y_train, epochs=10, batch_size=32)

# Evaluate the model
predictions = model.predict(x_test)
accuracy = np.mean(predictions == np.argmax(y_test, axis=1))
print(f"Test Accuracy: {accuracy * 100:.2f}%")

import numpy as np
from sklearn.preprocessing import OneHotEncoder
from keras.datasets import fashion_mnist

# Load the Fashion-MNIST dataset
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Normalize pixel values to [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Flatten the 28x28 images into 784-dimensional vectors
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# One-hot encode the labels
encoder = OneHotEncoder(sparse_output=False)
y_train = encoder.fit_transform(y_train.reshape(-1, 1))
y_test = encoder.transform(y_test.reshape(-1, 1))

# Define the neural network
class FeedforwardNeuralNetwork:
    def __init__(self, layer_sizes, learning_rate=0.01, optimizer='sgd', **optimizer_params):
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.optimizer = optimizer
        self.optimizer_params = optimizer_params

        # Initialize weights and biases
        self.weights = []
        self.biases = []
        for i in range(len(layer_sizes) - 1):
            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01)
            self.biases.append(np.zeros((1, layer_sizes[i + 1])))

        # Initialize optimizer-specific parameters
        self.initialize_optimizer()

    def initialize_optimizer(self):
        if self.optimizer == 'momentum':
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
        elif self.optimizer == 'nesterov':
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
        elif self.optimizer == 'rmsprop':
            self.s_weights = [np.zeros_like(w) for w in self.weights]
            self.s_biases = [np.zeros_like(b) for b in self.biases]
        elif self.optimizer == 'adam':
            self.m_weights = [np.zeros_like(w) for w in self.weights]
            self.m_biases = [np.zeros_like(b) for b in self.biases]
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
            self.t = 0
        elif self.optimizer == 'nadam':
            self.m_weights = [np.zeros_like(w) for w in self.weights]
            self.m_biases = [np.zeros_like(b) for b in self.biases]
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
            self.t = 0

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def forward(self, x):
        self.activations = [x]
        self.z_values = []

        for i in range(len(self.weights)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)
            if i == len(self.weights) - 1:
                # Output layer uses softmax
                activation = self.softmax(z)
            else:
                # Hidden layers use sigmoid
                activation = self.sigmoid(z)
            self.activations.append(activation)

        return self.activations[-1]

    def backward(self, x, y):
        m = x.shape[0]
        self.deltas = [None] * len(self.weights)

        # Output layer error
        output_error = self.activations[-1] - y
        self.deltas[-1] = output_error

        # Backpropagate errors
        for i in range(len(self.weights) - 2, -1, -1):
            error = np.dot(self.deltas[i + 1], self.weights[i + 1].T)
            self.deltas[i] = error * self.sigmoid_derivative(self.activations[i + 1])

        # Update weights and biases using the chosen optimizer
        self.update_parameters(m)

    def update_parameters(self, m):
        if self.optimizer == 'sgd':
            for i in range(len(self.weights)):
                self.weights[i] -= self.learning_rate * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.biases[i] -= self.learning_rate * np.sum(self.deltas[i], axis=0, keepdims=True) / m
        elif self.optimizer == 'momentum':
            beta = self.optimizer_params.get('beta', 0.9)
            for i in range(len(self.weights)):
                self.v_weights[i] = beta * self.v_weights[i] + (1 - beta) * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.v_biases[i] = beta * self.v_biases[i] + (1 - beta) * np.sum(self.deltas[i], axis=0, keepdims=True) / m
                self.weights[i] -= self.learning_rate * self.v_weights[i]
                self.biases[i] -= self.learning_rate * self.v_biases[i]
        elif self.optimizer == 'nesterov':
            beta = self.optimizer_params.get('beta', 0.9)
            for i in range(len(self.weights)):
                v_weights_prev = self.v_weights[i]
                v_biases_prev = self.v_biases[i]
                self.v_weights[i] = beta * self.v_weights[i] + (1 - beta) * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.v_biases[i] = beta * self.v_biases[i] + (1 - beta) * np.sum(self.deltas[i], axis=0, keepdims=True) / m
                self.weights[i] -= self.learning_rate * (beta * v_weights_prev + (1 - beta) * self.v_weights[i])
                self.biases[i] -= self.learning_rate * (beta * v_biases_prev + (1 - beta) * self.v_biases[i])
        elif self.optimizer == 'rmsprop':
            beta = self.optimizer_params.get('beta', 0.9)
            epsilon = self.optimizer_params.get('epsilon', 1e-8)
            for i in range(len(self.weights)):
                self.s_weights[i] = beta * self.s_weights[i] + (1 - beta) * np.square(np.dot(self.activations[i].T, self.deltas[i]) / m)
                self.s_biases[i] = beta * self.s_biases[i] + (1 - beta) * np.square(np.sum(self.deltas[i], axis=0, keepdims=True) / m)
                self.weights[i] -= self.learning_rate * (np.dot(self.activations[i].T, self.deltas[i]) / m) / (np.sqrt(self.s_weights[i]) + epsilon)
                self.biases[i] -= self.learning_rate * (np.sum(self.deltas[i], axis=0, keepdims=True) / m) / (np.sqrt(self.s_biases[i]) + epsilon)
        elif self.optimizer == 'adam':
            beta1 = self.optimizer_params.get('beta1', 0.9)
            beta2 = self.optimizer_params.get('beta2', 0.999)
            epsilon = self.optimizer_params.get('epsilon', 1e-8)
            self.t += 1
            for i in range(len(self.weights)):
                self.m_weights[i] = beta1 * self.m_weights[i] + (1 - beta1) * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.m_biases[i] = beta1 * self.m_biases[i] + (1 - beta1) * np.sum(self.deltas[i], axis=0, keepdims=True) / m
                self.v_weights[i] = beta2 * self.v_weights[i] + (1 - beta2) * np.square(np.dot(self.activations[i].T, self.deltas[i]) / m)
                self.v_biases[i] = beta2 * self.v_biases[i] + (1 - beta2) * np.square(np.sum(self.deltas[i], axis=0, keepdims=True) / m)
                m_weights_hat = self.m_weights[i] / (1 - beta1 ** self.t)
                m_biases_hat = self.m_biases[i] / (1 - beta1 ** self.t)
                v_weights_hat = self.v_weights[i] / (1 - beta2 ** self.t)
                v_biases_hat = self.v_biases[i] / (1 - beta2 ** self.t)
                self.weights[i] -= self.learning_rate * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)
                self.biases[i] -= self.learning_rate * m_biases_hat / (np.sqrt(v_biases_hat) + epsilon)
        elif self.optimizer == 'nadam':
            beta1 = self.optimizer_params.get('beta1', 0.9)
            beta2 = self.optimizer_params.get('beta2', 0.999)
            epsilon = self.optimizer_params.get('epsilon', 1e-8)
            self.t += 1
            for i in range(len(self.weights)):
                self.m_weights[i] = beta1 * self.m_weights[i] + (1 - beta1) * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.m_biases[i] = beta1 * self.m_biases[i] + (1 - beta1) * np.sum(self.deltas[i], axis=0, keepdims=True) / m
                self.v_weights[i] = beta2 * self.v_weights[i] + (1 - beta2) * np.square(np.dot(self.activations[i].T, self.deltas[i]) / m)
                self.v_biases[i] = beta2 * self.v_biases[i] + (1 - beta2) * np.square(np.sum(self.deltas[i], axis=0, keepdims=True) / m)
                m_weights_hat = (beta1 * self.m_weights[i] / (1 - beta1 ** self.t)) + ((1 - beta1) * np.dot(self.activations[i].T, self.deltas[i]) / m) / (1 - beta1 ** self.t)
                m_biases_hat = (beta1 * self.m_biases[i] / (1 - beta1 ** self.t)) + ((1 - beta1) * np.sum(self.deltas[i], axis=0, keepdims=True) / m) / (1 - beta1 ** self.t)
                v_weights_hat = self.v_weights[i] / (1 - beta2 ** self.t)
                v_biases_hat = self.v_biases[i] / (1 - beta2 ** self.t)
                self.weights[i] -= self.learning_rate * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)
                self.biases[i] -= self.learning_rate * m_biases_hat / (np.sqrt(v_biases_hat) + epsilon)

    def train(self, x, y, epochs=10, batch_size=32):
        for epoch in range(epochs):
            for i in range(0, x.shape[0], batch_size):
                x_batch = x[i:i + batch_size]
                y_batch = y[i:i + batch_size]

                # Forward pass
                self.forward(x_batch)

                # Backward pass
                self.backward(x_batch, y_batch)

            # Print loss every epoch
            predictions = self.forward(x)
            loss = -np.mean(y * np.log(predictions + 1e-10))
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}")

    def predict(self, x):
        return np.argmax(self.forward(x), axis=1)

# Example usage
layer_sizes = [784, 128, 64, 10]  # Input layer: 784, Hidden layers: 128 and 64, Output layer: 10
optimizer = 'sgd'  # Choose from 'sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam'
model = FeedforwardNeuralNetwork(layer_sizes, learning_rate=0.001, optimizer=optimizer, beta1=0.9, beta2=0.999, epsilon=1e-8)

# Train the model
model.train(x_train, y_train, epochs=10, batch_size=32)

# Evaluate the model
predictions = model.predict(x_test)
accuracy = np.mean(predictions == np.argmax(y_test, axis=1))
print(f"Test Accuracy: {accuracy * 100:.2f}%")

import numpy as np
from sklearn.preprocessing import OneHotEncoder
from keras.datasets import fashion_mnist

# Load the Fashion-MNIST dataset
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Normalize pixel values to [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Flatten the 28x28 images into 784-dimensional vectors
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# One-hot encode the labels
encoder = OneHotEncoder(sparse_output=False)
y_train = encoder.fit_transform(y_train.reshape(-1, 1))
y_test = encoder.transform(y_test.reshape(-1, 1))

# Define the neural network
class FeedforwardNeuralNetwork:
    def __init__(self, layer_sizes, learning_rate=0.01, optimizer='sgd', **optimizer_params):
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.optimizer = optimizer
        self.optimizer_params = optimizer_params

        # Initialize weights and biases
        self.weights = []
        self.biases = []
        for i in range(len(layer_sizes) - 1):
            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.01)
            self.biases.append(np.zeros((1, layer_sizes[i + 1])))

        # Initialize optimizer-specific parameters
        self.initialize_optimizer()

    def initialize_optimizer(self):
        if self.optimizer == 'momentum':
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
        elif self.optimizer == 'nesterov':
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
        elif self.optimizer == 'rmsprop':
            self.s_weights = [np.zeros_like(w) for w in self.weights]
            self.s_biases = [np.zeros_like(b) for b in self.biases]
        elif self.optimizer == 'adam':
            self.m_weights = [np.zeros_like(w) for w in self.weights]
            self.m_biases = [np.zeros_like(b) for b in self.biases]
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
            self.t = 0
        elif self.optimizer == 'nadam':
            self.m_weights = [np.zeros_like(w) for w in self.weights]
            self.m_biases = [np.zeros_like(b) for b in self.biases]
            self.v_weights = [np.zeros_like(w) for w in self.weights]
            self.v_biases = [np.zeros_like(b) for b in self.biases]
            self.t = 0

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        return x * (1 - x)

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def forward(self, x):
        self.activations = [x]
        self.z_values = []

        for i in range(len(self.weights)):
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)
            if i == len(self.weights) - 1:
                # Output layer uses softmax
                activation = self.softmax(z)
            else:
                # Hidden layers use sigmoid
                activation = self.sigmoid(z)
            self.activations.append(activation)

        return self.activations[-1]

    def backward(self, x, y):
        m = x.shape[0]
        self.deltas = [None] * len(self.weights)

        # Output layer error
        output_error = self.activations[-1] - y
        self.deltas[-1] = output_error

        # Backpropagate errors
        for i in range(len(self.weights) - 2, -1, -1):
            error = np.dot(self.deltas[i + 1], self.weights[i + 1].T)
            self.deltas[i] = error * self.sigmoid_derivative(self.activations[i + 1])

        # Update weights and biases using the chosen optimizer
        self.update_parameters(m)

    def update_parameters(self, m):
        if self.optimizer == 'sgd':
            for i in range(len(self.weights)):
                self.weights[i] -= self.learning_rate * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.biases[i] -= self.learning_rate * np.sum(self.deltas[i], axis=0, keepdims=True) / m
        elif self.optimizer == 'momentum':
            beta = self.optimizer_params.get('beta', 0.9)
            for i in range(len(self.weights)):
                self.v_weights[i] = beta * self.v_weights[i] + (1 - beta) * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.v_biases[i] = beta * self.v_biases[i] + (1 - beta) * np.sum(self.deltas[i], axis=0, keepdims=True) / m
                self.weights[i] -= self.learning_rate * self.v_weights[i]
                self.biases[i] -= self.learning_rate * self.v_biases[i]
        elif self.optimizer == 'nesterov':
            beta = self.optimizer_params.get('beta', 0.9)
            for i in range(len(self.weights)):
                v_weights_prev = self.v_weights[i]
                v_biases_prev = self.v_biases[i]
                self.v_weights[i] = beta * self.v_weights[i] + (1 - beta) * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.v_biases[i] = beta * self.v_biases[i] + (1 - beta) * np.sum(self.deltas[i], axis=0, keepdims=True) / m
                self.weights[i] -= self.learning_rate * (beta * v_weights_prev + (1 - beta) * self.v_weights[i])
                self.biases[i] -= self.learning_rate * (beta * v_biases_prev + (1 - beta) * self.v_biases[i])
        elif self.optimizer == 'rmsprop':
            beta = self.optimizer_params.get('beta', 0.9)
            epsilon = self.optimizer_params.get('epsilon', 1e-8)
            for i in range(len(self.weights)):
                self.s_weights[i] = beta * self.s_weights[i] + (1 - beta) * np.square(np.dot(self.activations[i].T, self.deltas[i]) / m)
                self.s_biases[i] = beta * self.s_biases[i] + (1 - beta) * np.square(np.sum(self.deltas[i], axis=0, keepdims=True) / m)
                self.weights[i] -= self.learning_rate * (np.dot(self.activations[i].T, self.deltas[i]) / m) / (np.sqrt(self.s_weights[i]) + epsilon)
                self.biases[i] -= self.learning_rate * (np.sum(self.deltas[i], axis=0, keepdims=True) / m) / (np.sqrt(self.s_biases[i]) + epsilon)
        elif self.optimizer == 'adam':
            beta1 = self.optimizer_params.get('beta1', 0.9)
            beta2 = self.optimizer_params.get('beta2', 0.999)
            epsilon = self.optimizer_params.get('epsilon', 1e-8)
            self.t += 1
            for i in range(len(self.weights)):
                self.m_weights[i] = beta1 * self.m_weights[i] + (1 - beta1) * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.m_biases[i] = beta1 * self.m_biases[i] + (1 - beta1) * np.sum(self.deltas[i], axis=0, keepdims=True) / m
                self.v_weights[i] = beta2 * self.v_weights[i] + (1 - beta2) * np.square(np.dot(self.activations[i].T, self.deltas[i]) / m)
                self.v_biases[i] = beta2 * self.v_biases[i] + (1 - beta2) * np.square(np.sum(self.deltas[i], axis=0, keepdims=True) / m)
                m_weights_hat = self.m_weights[i] / (1 - beta1 ** self.t)
                m_biases_hat = self.m_biases[i] / (1 - beta1 ** self.t)
                v_weights_hat = self.v_weights[i] / (1 - beta2 ** self.t)
                v_biases_hat = self.v_biases[i] / (1 - beta2 ** self.t)
                self.weights[i] -= self.learning_rate * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)
                self.biases[i] -= self.learning_rate * m_biases_hat / (np.sqrt(v_biases_hat) + epsilon)
        elif self.optimizer == 'nadam':
            beta1 = self.optimizer_params.get('beta1', 0.9)
            beta2 = self.optimizer_params.get('beta2', 0.999)
            epsilon = self.optimizer_params.get('epsilon', 1e-8)
            self.t += 1
            for i in range(len(self.weights)):
                self.m_weights[i] = beta1 * self.m_weights[i] + (1 - beta1) * np.dot(self.activations[i].T, self.deltas[i]) / m
                self.m_biases[i] = beta1 * self.m_biases[i] + (1 - beta1) * np.sum(self.deltas[i], axis=0, keepdims=True) / m
                self.v_weights[i] = beta2 * self.v_weights[i] + (1 - beta2) * np.square(np.dot(self.activations[i].T, self.deltas[i]) / m)
                self.v_biases[i] = beta2 * self.v_biases[i] + (1 - beta2) * np.square(np.sum(self.deltas[i], axis=0, keepdims=True) / m)
                m_weights_hat = (beta1 * self.m_weights[i] / (1 - beta1 ** self.t)) + ((1 - beta1) * np.dot(self.activations[i].T, self.deltas[i]) / m) / (1 - beta1 ** self.t)
                m_biases_hat = (beta1 * self.m_biases[i] / (1 - beta1 ** self.t)) + ((1 - beta1) * np.sum(self.deltas[i], axis=0, keepdims=True) / m) / (1 - beta1 ** self.t)
                v_weights_hat = self.v_weights[i] / (1 - beta2 ** self.t)
                v_biases_hat = self.v_biases[i] / (1 - beta2 ** self.t)
                self.weights[i] -= self.learning_rate * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)
                self.biases[i] -= self.learning_rate * m_biases_hat / (np.sqrt(v_biases_hat) + epsilon)

    def train(self, x, y, epochs=10, batch_size=32):
        for epoch in range(epochs):
            for i in range(0, x.shape[0], batch_size):
                x_batch = x[i:i + batch_size]
                y_batch = y[i:i + batch_size]

                # Forward pass
                self.forward(x_batch)

                # Backward pass
                self.backward(x_batch, y_batch)

            # Print loss every epoch
            predictions = self.forward(x)
            loss = -np.mean(y * np.log(predictions + 1e-10))
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}")

    def predict(self, x):
        return np.argmax(self.forward(x), axis=1)

# Example usage
layer_sizes = [784, 128, 64, 10]  # Input layer: 784, Hidden layers: 128 and 64, Output layer: 10
optimizer = 'momentum'  # Choose from 'sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam'
model = FeedforwardNeuralNetwork(layer_sizes, learning_rate=0.001, optimizer=optimizer, beta1=0.9, beta2=0.999, epsilon=1e-8)

# Train the model
model.train(x_train, y_train, epochs=10, batch_size=32)

# Evaluate the model
predictions = model.predict(x_test)
accuracy = np.mean(predictions == np.argmax(y_test, axis=1))
print(f"Test Accuracy: {accuracy * 100:.2f}%")