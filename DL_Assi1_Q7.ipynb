{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5AQr72rl256VpuFPkbj/c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manglesh001/DL-assigment1/blob/main/DL_Assi1_Q7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import wandb"
      ],
      "metadata": {
        "id": "08CcI9x9czkf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the best model parameters\n",
        "best_config = {\n",
        "    'hidden_layers': 3,\n",
        "    'hidden_size': 128,\n",
        "    'activation': 'relu',\n",
        "    'weight_init': 'xavier',\n",
        "    'optimizer': 'rmsprop',\n",
        "    'batch_size': 16,\n",
        "    'epochs': 5,\n",
        "    'learning_rate': 0.001,\n",
        "    'weight_decay': 0\n",
        "}"
      ],
      "metadata": {
        "id": "VaDhHtbvdogi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Wandb\n",
        "wandb.init(project=\"fashion-mnist\", config=best_config)\n",
        "\n",
        "# Activation functions and derivatives\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)"
      ],
      "metadata": {
        "id": "rKqGhCAxdq6w"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weight Initialization\n",
        "def initialize_weights(layers, method=\"xavier\"):\n",
        "    weights = []\n",
        "    biases = []\n",
        "    for i in range(len(layers) - 1):\n",
        "        if method == \"xavier\":\n",
        "            weights.append(np.random.randn(layers[i], layers[i+1]) * np.sqrt(1 / layers[i]))\n",
        "        else:  # random\n",
        "            weights.append(np.random.randn(layers[i], layers[i+1]) * 0.01)\n",
        "        biases.append(np.zeros((1, layers[i+1])))\n",
        "    return weights, biases\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b62zkKmrduJk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward Propagation\n",
        "def forward_propagation(X, weights, biases, activation):\n",
        "    activations = [X]\n",
        "    for i in range(len(weights)):\n",
        "        z = np.dot(activations[-1], weights[i]) + biases[i]\n",
        "        if activation[i] == \"relu\":\n",
        "            activations.append(relu(z))\n",
        "        elif activation[i] == \"sigmoid\":\n",
        "            activations.append(sigmoid(z))\n",
        "    return activations\n"
      ],
      "metadata": {
        "id": "V8K7SeD5dwG-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation\n",
        "def backpropagation(y, activations, weights, activation):\n",
        "    gradients_w = [None] * len(weights)\n",
        "    gradients_b = [None] * len(weights)\n",
        "    error = activations[-1] - y\n",
        "\n",
        "    for i in reversed(range(len(weights))):\n",
        "        if activation[i] == \"relu\":\n",
        "            delta = error * relu_derivative(activations[i+1])\n",
        "        elif activation[i] == \"sigmoid\":\n",
        "            delta = error * sigmoid_derivative(activations[i+1])\n",
        "        gradients_w[i] = np.dot(activations[i].T, delta)\n",
        "        gradients_b[i] = np.sum(delta, axis=0, keepdims=True)\n",
        "        error = np.dot(delta, weights[i].T)\n",
        "\n",
        "    return gradients_w, gradients_b"
      ],
      "metadata": {
        "id": "-UoFD9vOdwTg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSprop Optimizer\n",
        "def rmsprop(weights, biases, gradients_w, gradients_b, lr, cache_w, cache_b, beta=0.99, epsilon=1e-8):\n",
        "    for i in range(len(weights)):\n",
        "        cache_w[i] = beta * cache_w[i] + (1 - beta) * (gradients_w[i] ** 2)\n",
        "        weights[i] -= lr * gradients_w[i] / (np.sqrt(cache_w[i]) + epsilon)\n",
        "        cache_b[i] = beta * cache_b[i] + (1 - beta) * (gradients_b[i] ** 2)\n",
        "        biases[i] -= lr * gradients_b[i] / (np.sqrt(cache_b[i]) + epsilon)\n",
        "    return weights, biases, cache_w, cache_b\n",
        "\n",
        "# Load Fashion-MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
        "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = np.eye(10)[y_train]\n",
        "y_test_original = y_test  # Keep original labels for confusion matrix"
      ],
      "metadata": {
        "id": "blPaIppDd0bO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_network(X_train, y_train, X_val, y_val, config):\n",
        "    np.random.seed(42)\n",
        "    layers = [X_train.shape[1]] + [config['hidden_size']] * config['hidden_layers'] + [10]\n",
        "    activation = [config['activation']] * config['hidden_layers'] + ['sigmoid']\n",
        "\n",
        "    weights, biases = initialize_weights(layers, config['weight_init'])\n",
        "    cache_w = [np.zeros_like(w) for w in weights]\n",
        "    cache_b = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    batch_size = config['batch_size']\n",
        "    epochs = config['epochs']\n",
        "    lr = config['learning_rate']\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(X_train.shape[0])\n",
        "        X_train_shuffled, y_train_shuffled = X_train[indices], y_train[indices]\n",
        "\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for i in range(0, X_train_shuffled.shape[0], batch_size):\n",
        "            X_batch = X_train_shuffled[i:i+batch_size]\n",
        "            y_batch = y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "            # Forward propagation\n",
        "            activations = forward_propagation(X_batch, weights, biases, activation)\n",
        "\n",
        "            # Calculate training loss (cross-entropy loss)\n",
        "            output = activations[-1]\n",
        "            train_loss += -np.sum(y_batch * np.log(output + 1e-8)) / len(y_batch)\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            train_preds = np.argmax(output, axis=1)\n",
        "            train_true = np.argmax(y_batch, axis=1)\n",
        "            train_correct += np.sum(train_preds == train_true)\n",
        "            train_total += len(y_batch)\n",
        "\n",
        "            # Backpropagation\n",
        "            gradients_w, gradients_b = backpropagation(y_batch, activations, weights, activation)\n",
        "            weights, biases, cache_w, cache_b = rmsprop(weights, biases, gradients_w, gradients_b, lr, cache_w, cache_b)\n",
        "\n",
        "        # Calculate average training loss and accuracy for the epoch\n",
        "        train_loss /= (X_train_shuffled.shape[0] // batch_size)\n",
        "        train_accuracy = train_correct / train_total\n",
        "\n",
        "        # Log training metrics to Wandb\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_accuracy\n",
        "        })\n",
        "\n",
        "        # Print training metrics\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VOJkD7K1eBrM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the best model\n",
        "X_train, X_val = X_train[:54000], X_train[54000:]\n",
        "y_train, y_val = y_train[:54000], y_train[54000:]\n",
        "weights, biases = train_network(X_train, y_train, X_val, y_val, best_config)\n",
        "\n",
        "# Evaluate on the test set\n",
        "activations = forward_propagation(X_test, weights, biases, [best_config['activation']] * best_config['hidden_layers'] + ['sigmoid'])\n",
        "test_predictions = np.argmax(activations[-1], axis=1)\n",
        "\n",
        "# One-hot encode y_test_original for loss calculation\n",
        "y_test_one_hot = np.eye(10)[y_test_original]\n",
        "\n",
        "# Calculate test loss (cross-entropy loss)\n",
        "test_loss = -np.sum(y_test_one_hot * np.log(activations[-1] + 1e-8)) / len(y_test_original)\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = np.mean(test_predictions == y_test_original)\n",
        "\n",
        "# Log test metrics to Wandb\n",
        "wandb.log({\n",
        "    \"test_loss\": test_loss,\n",
        "    \"test_accuracy\": test_accuracy\n",
        "})\n",
        "\n",
        "# Print test metrics\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57PCiHu5d570",
        "outputId": "45bb6403-911a-4879-9252-39e3090db73c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.6836, Train Accuracy: 0.7995\n",
            "Epoch 2/5, Train Loss: 0.4161, Train Accuracy: 0.8607\n",
            "Epoch 3/5, Train Loss: 0.3894, Train Accuracy: 0.8736\n",
            "Epoch 4/5, Train Loss: 0.3744, Train Accuracy: 0.8807\n",
            "Epoch 5/5, Train Loss: 0.3677, Train Accuracy: 0.8884\n",
            "Test Loss: 0.4300, Test Accuracy: 0.8700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBHw_aBXcx3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test_original, test_predictions)\n",
        "\n",
        "# Plot confusion matrix\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Fashion-MNIST Test Set', fontsize=16)\n",
        "plt.xlabel('Predicted Labels', fontsize=14)\n",
        "plt.ylabel('True Labels', fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Save the confusion matrix plot to a file\n",
        "confusion_matrix_path = \"confusion_matrix.png\"\n",
        "plt.savefig(confusion_matrix_path)\n",
        "plt.close()\n",
        "\n",
        "# Log the confusion matrix as an image to Wandb\n",
        "wandb.log({\"confusion_matrix\": wandb.Image(confusion_matrix_path)})\n",
        "\n",
        "# Finish Wandb run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "3c9VBHIycmOL",
        "outputId": "348da611-2f77-42de-8f07-dbed6809c472"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇█</td></tr><tr><td>train_loss</td><td>█▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>test_accuracy</td><td>0.87</td></tr><tr><td>test_loss</td><td>0.42999</td></tr><tr><td>train_accuracy</td><td>0.88844</td></tr><tr><td>train_loss</td><td>0.36766</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">worldly-cherry-483</strong> at: <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/fashion-mnist/runs/gpwiugwt' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/fashion-mnist/runs/gpwiugwt</a><br> View project at: <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/fashion-mnist' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/fashion-mnist</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250317_141421-gpwiugwt/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}