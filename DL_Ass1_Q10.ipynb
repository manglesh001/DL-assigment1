{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN80kmbxItxoiyj2QjTxNVw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manglesh001/DL-assigment1/blob/main/DL_Ass1_Q10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import wandb"
      ],
      "metadata": {
        "id": "t4bwQijTipfZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize Wandb\n",
        "wandb.init(project=\"mnist\", config={})\n",
        "\n",
        "# Activation functions and derivatives\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "# Activation functions and derivatives\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)"
      ],
      "metadata": {
        "id": "IjDFlWaYisRC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Weight Initialization\n",
        "def initialize_weights(layers, method=\"xavier\"):\n",
        "    weights = []\n",
        "    biases = []\n",
        "    for i in range(len(layers) - 1):\n",
        "        if method == \"xavier\":\n",
        "            weights.append(np.random.randn(layers[i], layers[i+1]) * np.sqrt(1 / layers[i]))\n",
        "        else:  # random\n",
        "            weights.append(np.random.randn(layers[i], layers[i+1]) * 0.01)\n",
        "        biases.append(np.zeros((1, layers[i+1])))\n",
        "    return weights, biases"
      ],
      "metadata": {
        "id": "HLGZsSJfisOV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Forward Propagation\n",
        "def forward_propagation(X, weights, biases, activation):\n",
        "    activations = [X]\n",
        "    for i in range(len(weights)):\n",
        "        z = np.dot(activations[-1], weights[i]) + biases[i]\n",
        "        if activation[i] == \"sigmoid\":\n",
        "            activations.append(sigmoid(z))\n",
        "        elif activation[i] == \"tanh\":\n",
        "            activations.append(tanh(z))\n",
        "        elif activation[i] == \"relu\":\n",
        "            activations.append(relu(z))\n",
        "    return activations\n"
      ],
      "metadata": {
        "id": "5BrSFrbEisLq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Backpropagation\n",
        "def backpropagation(y, activations, weights, activation):\n",
        "    gradients_w = [None] * len(weights)\n",
        "    gradients_b = [None] * len(weights)\n",
        "    error = activations[-1] - y\n",
        "\n",
        "    for i in reversed(range(len(weights))):\n",
        "        if activation[i] == \"sigmoid\":\n",
        "            delta = error * sigmoid_derivative(activations[i+1])\n",
        "        elif activation[i] == \"tanh\":\n",
        "            delta = error * tanh_derivative(activations[i+1])\n",
        "        elif activation[i] == \"relu\":\n",
        "            delta = error * relu_derivative(activations[i+1])\n",
        "        gradients_w[i] = np.dot(activations[i].T, delta)\n",
        "        gradients_b[i] = np.sum(delta, axis=0, keepdims=True)\n",
        "        error = np.dot(delta, weights[i].T)\n",
        "\n",
        "    return gradients_w, gradients_b"
      ],
      "metadata": {
        "id": "6vhmXQHIisH0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSprop Optimizer\n",
        "def rmsprop(weights, biases, gradients_w, gradients_b, lr, cache_w, cache_b, beta=0.99, epsilon=1e-8):\n",
        "    for i in range(len(weights)):\n",
        "        cache_w[i] = beta * cache_w[i] + (1 - beta) * (gradients_w[i] ** 2)\n",
        "        weights[i] -= lr * gradients_w[i] / (np.sqrt(cache_w[i]) + epsilon)\n",
        "        cache_b[i] = beta * cache_b[i] + (1 - beta) * (gradients_b[i] ** 2)\n",
        "        biases[i] -= lr * gradients_b[i] / (np.sqrt(cache_b[i]) + epsilon)\n",
        "    return weights, biases, cache_w, cache_b\n",
        "\n"
      ],
      "metadata": {
        "id": "VaLAXJX1isFA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam Optimizer\n",
        "def adam(weights, biases, gradients_w, gradients_b, lr, m_w, v_w, m_b, v_b, beta1=0.9, beta2=0.999, epsilon=1e-8, t=1):\n",
        "    for i in range(len(weights)):\n",
        "        # Update momentum and velocity for weights\n",
        "        m_w[i] = beta1 * m_w[i] + (1 - beta1) * gradients_w[i]\n",
        "        v_w[i] = beta2 * v_w[i] + (1 - beta2) * (gradients_w[i] ** 2)\n",
        "        m_w_hat = m_w[i] / (1 - beta1 ** t)\n",
        "        v_w_hat = v_w[i] / (1 - beta2 ** t)\n",
        "        weights[i] -= lr * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
        "\n",
        "        # Update momentum and velocity for biases\n",
        "        m_b[i] = beta1 * m_b[i] + (1 - beta1) * gradients_b[i]\n",
        "        v_b[i] = beta2 * v_b[i] + (1 - beta2) * (gradients_b[i] ** 2)\n",
        "        m_b_hat = m_b[i] / (1 - beta1 ** t)\n",
        "        v_b_hat = v_b[i] / (1 - beta2 ** t)\n",
        "        biases[i] -= lr * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "    return weights, biases, m_w, v_w, m_b, v_b"
      ],
      "metadata": {
        "id": "8iZxnV2uisCL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
        "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = np.eye(10)[y_train]\n",
        "y_test_original = y_test  # Keep original labels for confusion matrix"
      ],
      "metadata": {
        "id": "KBXxgcLlir_4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define configurations\n",
        "configurations = [\n",
        "    {\n",
        "        'hidden_layers': 3,\n",
        "        'hidden_size': 128,\n",
        "        'activation': 'relu',\n",
        "        'weight_init': 'xavier',\n",
        "        'optimizer': 'rmsprop',\n",
        "        'batch_size': 16,\n",
        "        'epochs': 5,\n",
        "        'learning_rate': 0.001,\n",
        "        'weight_decay': 0\n",
        "    },\n",
        "    {\n",
        "        'hidden_layers': 5,\n",
        "        'hidden_size': 128,\n",
        "        'activation': 'relu',\n",
        "        'weight_init': 'xavier',\n",
        "        'optimizer': 'adam',\n",
        "        'batch_size': 32,\n",
        "        'epochs': 5,\n",
        "        'learning_rate': 0.001,\n",
        "        'weight_decay': 0.5\n",
        "    },\n",
        "    {\n",
        "        'hidden_layers': 4,\n",
        "        'hidden_size': 128,\n",
        "        'activation': 'tanh',\n",
        "        'weight_init': 'xavier',\n",
        "        'optimizer': 'adam',\n",
        "        'batch_size': 16,\n",
        "        'epochs': 10,\n",
        "        'learning_rate': 0.001,\n",
        "        'weight_decay': 0.5\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "snZlppKLiDL_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2KV6FGXMhcE9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_network(X_train, y_train, X_val, y_val, config):\n",
        "    np.random.seed(42)\n",
        "    layers = [X_train.shape[1]] + [config['hidden_size']] * config['hidden_layers'] + [10]\n",
        "    activation = [config['activation']] * config['hidden_layers'] + ['sigmoid']\n",
        "\n",
        "    weights, biases = initialize_weights(layers, config['weight_init'])\n",
        "    cache_w = [np.zeros_like(w) for w in weights]\n",
        "    cache_b = [np.zeros_like(b) for b in biases]\n",
        "    m_w = [np.zeros_like(w) for w in weights]\n",
        "    v_w = [np.zeros_like(w) for w in weights]\n",
        "    m_b = [np.zeros_like(b) for b in biases]\n",
        "    v_b = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    batch_size = config['batch_size']\n",
        "    epochs = config['epochs']\n",
        "    lr = config['learning_rate']\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(X_train.shape[0])\n",
        "        X_train_shuffled, y_train_shuffled = X_train[indices], y_train[indices]\n",
        "\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for i in range(0, X_train_shuffled.shape[0], batch_size):\n",
        "            X_batch = X_train_shuffled[i:i+batch_size]\n",
        "            y_batch = y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "            # Forward propagation\n",
        "            activations = forward_propagation(X_batch, weights, biases, activation)\n",
        "\n",
        "            # Calculate training loss (cross-entropy loss)\n",
        "            output = activations[-1]\n",
        "            train_loss += -np.sum(y_batch * np.log(output + 1e-8)) / len(y_batch)\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            train_preds = np.argmax(output, axis=1)\n",
        "            train_true = np.argmax(y_batch, axis=1)\n",
        "            train_correct += np.sum(train_preds == train_true)\n",
        "            train_total += len(y_batch)\n",
        "\n",
        "            # Backpropagation\n",
        "            gradients_w, gradients_b = backpropagation(y_batch, activations, weights, activation)\n",
        "            if config['optimizer'] == \"rmsprop\":\n",
        "                weights, biases, cache_w, cache_b = rmsprop(weights, biases, gradients_w, gradients_b, lr, cache_w, cache_b)\n",
        "            elif config['optimizer'] == \"adam\":\n",
        "                weights, biases, m_w, v_w, m_b, v_b = adam(weights, biases, gradients_w, gradients_b, lr, m_w, v_w, m_b, v_b, t=epoch+1)\n",
        "\n",
        "        # Calculate average training loss and accuracy for the epoch\n",
        "        train_loss /= (X_train_shuffled.shape[0] // batch_size)\n",
        "        train_accuracy = train_correct / train_total\n",
        "\n",
        "        # Validation\n",
        "        val_activations = forward_propagation(X_val, weights, biases, activation)\n",
        "        val_loss = -np.sum(y_val * np.log(val_activations[-1] + 1e-8)) / len(y_val)\n",
        "        val_preds = np.argmax(val_activations[-1], axis=1)\n",
        "        val_true = np.argmax(y_val, axis=1)\n",
        "        val_accuracy = np.mean(val_preds == val_true)\n",
        "\n",
        "        # Log metrics to Wandb\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_accuracy,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_accuracy\n",
        "        })\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "# Split training data into training and validation sets\n",
        "X_train, X_val = X_train[:50000], X_train[50000:]\n",
        "y_train, y_val = y_train[:50000], y_train[50000:]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate each configuration\n",
        "for i, config in enumerate(configurations):\n",
        "    print(f\"\\nTraining Configuration {i + 1}: {config}\")\n",
        "    wandb.init(project=\"mnist\", config=config, reinit=True)\n",
        "    weights, biases = train_network(X_train, y_train, X_val, y_val, config)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    activations = forward_propagation(X_test, weights, biases, [config['activation']] * config['hidden_layers'] + ['sigmoid'])\n",
        "    test_predictions = np.argmax(activations[-1], axis=1)\n",
        "\n",
        "    # Calculate test loss (cross-entropy loss)\n",
        "    test_loss = -np.sum(np.eye(10)[y_test_original] * np.log(activations[-1] + 1e-8)) / len(y_test_original)\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    test_accuracy = np.mean(test_predictions == y_test_original)\n",
        "\n",
        "    # Log test metrics to Wandb\n",
        "    wandb.log({\n",
        "        \"test_loss\": test_loss,\n",
        "        \"test_accuracy\": test_accuracy\n",
        "    })\n",
        "\n",
        "    # Print test metrics\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LbSPcpnCh8kh",
        "outputId": "9e23ecc2-fb8f-43d1-ca07-ca76cf9f7614"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Configuration 1: {'hidden_layers': 3, 'hidden_size': 128, 'activation': 'relu', 'weight_init': 'xavier', 'optimizer': 'rmsprop', 'batch_size': 16, 'epochs': 5, 'learning_rate': 0.001, 'weight_decay': 0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">legendary-universe-5</strong> at: <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/u56qlg36' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/u56qlg36</a><br> View project at: <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250317_143716-u56qlg36/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250317_143719-qrel905v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/qrel905v' target=\"_blank\">vocal-sound-6</a></strong> to <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/qrel905v' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/qrel905v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.2666, Train Accuracy: 0.9246, Val Loss: 0.1502, Val Accuracy: 0.9619\n",
            "Epoch 2/5, Train Loss: 0.1378, Train Accuracy: 0.9635, Val Loss: 0.1341, Val Accuracy: 0.9688\n",
            "Epoch 3/5, Train Loss: 0.1072, Train Accuracy: 0.9727, Val Loss: 0.1406, Val Accuracy: 0.9654\n",
            "Epoch 4/5, Train Loss: 0.0973, Train Accuracy: 0.9777, Val Loss: 0.1302, Val Accuracy: 0.9724\n",
            "Epoch 5/5, Train Loss: 0.0888, Train Accuracy: 0.9810, Val Loss: 0.1304, Val Accuracy: 0.9735\n",
            "Test Loss: 0.1269, Test Accuracy: 0.9718\n",
            "\n",
            "Training Configuration 2: {'hidden_layers': 5, 'hidden_size': 128, 'activation': 'relu', 'weight_init': 'xavier', 'optimizer': 'adam', 'batch_size': 32, 'epochs': 5, 'learning_rate': 0.001, 'weight_decay': 0.5}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁▆▇██</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▃▇█</td></tr><tr><td>val_loss</td><td>█▂▅▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>test_accuracy</td><td>0.9718</td></tr><tr><td>test_loss</td><td>0.12689</td></tr><tr><td>train_accuracy</td><td>0.98104</td></tr><tr><td>train_loss</td><td>0.08877</td></tr><tr><td>val_accuracy</td><td>0.9735</td></tr><tr><td>val_loss</td><td>0.13038</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vocal-sound-6</strong> at: <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/qrel905v' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/qrel905v</a><br> View project at: <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250317_143719-qrel905v/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250317_143827-4rqr82gp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/4rqr82gp' target=\"_blank\">hardy-universe-7</a></strong> to <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/4rqr82gp' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/4rqr82gp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 18.2913, Train Accuracy: 0.1133, Val Loss: 18.3975, Val Accuracy: 0.1064\n",
            "Epoch 2/5, Train Loss: 18.3576, Train Accuracy: 0.1136, Val Loss: 18.4087, Val Accuracy: 0.1064\n",
            "Epoch 3/5, Train Loss: 18.4198, Train Accuracy: 0.1136, Val Loss: 18.4085, Val Accuracy: 0.1064\n",
            "Epoch 4/5, Train Loss: 18.4196, Train Accuracy: 0.1136, Val Loss: 18.4084, Val Accuracy: 0.1064\n",
            "Epoch 5/5, Train Loss: 18.4194, Train Accuracy: 0.1136, Val Loss: 18.4082, Val Accuracy: 0.1064\n",
            "Test Loss: 18.4066, Test Accuracy: 0.1135\n",
            "\n",
            "Training Configuration 3: {'hidden_layers': 4, 'hidden_size': 128, 'activation': 'tanh', 'weight_init': 'xavier', 'optimizer': 'adam', 'batch_size': 16, 'epochs': 10, 'learning_rate': 0.001, 'weight_decay': 0.5}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁████</td></tr><tr><td>train_loss</td><td>▁▅███</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>test_accuracy</td><td>0.1135</td></tr><tr><td>test_loss</td><td>18.40665</td></tr><tr><td>train_accuracy</td><td>0.11356</td></tr><tr><td>train_loss</td><td>18.41942</td></tr><tr><td>val_accuracy</td><td>0.1064</td></tr><tr><td>val_loss</td><td>18.40823</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">hardy-universe-7</strong> at: <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/4rqr82gp' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/4rqr82gp</a><br> View project at: <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250317_143827-4rqr82gp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250317_143951-0hw3lyhb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/0hw3lyhb' target=\"_blank\">flowing-microwave-8</a></strong> to <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/0hw3lyhb' target=\"_blank\">https://wandb.ai/mangleshpatidar2233-iit-madras-alumni-association/mnist/runs/0hw3lyhb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.3392, Train Accuracy: 0.9031, Val Loss: 0.2298, Val Accuracy: 0.9451\n",
            "Epoch 2/10, Train Loss: 0.2008, Train Accuracy: 0.9460, Val Loss: 0.1644, Val Accuracy: 0.9551\n",
            "Epoch 3/10, Train Loss: 0.1661, Train Accuracy: 0.9562, Val Loss: 0.1488, Val Accuracy: 0.9598\n",
            "Epoch 4/10, Train Loss: 0.1431, Train Accuracy: 0.9630, Val Loss: 0.1349, Val Accuracy: 0.9625\n",
            "Epoch 5/10, Train Loss: 0.1282, Train Accuracy: 0.9674, Val Loss: 0.1314, Val Accuracy: 0.9642\n",
            "Epoch 6/10, Train Loss: 0.1159, Train Accuracy: 0.9713, Val Loss: 0.1370, Val Accuracy: 0.9621\n",
            "Epoch 7/10, Train Loss: 0.1060, Train Accuracy: 0.9740, Val Loss: 0.1160, Val Accuracy: 0.9687\n",
            "Epoch 8/10, Train Loss: 0.0991, Train Accuracy: 0.9753, Val Loss: 0.1097, Val Accuracy: 0.9696\n",
            "Epoch 9/10, Train Loss: 0.0924, Train Accuracy: 0.9779, Val Loss: 0.1132, Val Accuracy: 0.9694\n",
            "Epoch 10/10, Train Loss: 0.0867, Train Accuracy: 0.9793, Val Loss: 0.1164, Val Accuracy: 0.9691\n",
            "Test Loss: 0.1225, Test Accuracy: 0.9650\n"
          ]
        }
      ]
    }
  ]
}